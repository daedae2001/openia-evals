# evaluaciones

Evals es un marco para evaluar modelos OpenAI y un registro de puntos de referencia de c贸digo abierto.

Puede usar Evals para crear y ejecutar evaluaciones que:
- usar conjuntos de datos para generar avisos,
- medir la calidad de las terminaciones proporcionadas por un modelo OpenAI, y
- comparar el rendimiento en diferentes conjuntos de datos y modelos.

Con Evals, nuestro objetivo es hacer que sea lo m谩s simple posible construir una evaluaci贸n mientras se escribe la menor cantidad de c贸digo posible. Para comenzar, le recomendamos que siga estos pasos **en orden**:
1. Lea este documento y siga las [instrucciones de configuraci贸n a continuaci贸n] (README.md#Setup).
2. Aprenda a ejecutar evaluaciones existentes: [run-evals.md](docs/run-evals.md).
3. Familiar铆cese con las plantillas de evaluaci贸n existentes: [eval-templates.md](docs/eval-templates.md).
4. Siga el proceso para crear una evaluaci贸n: [build-eval.md](docs/build-eval.md)
5. Vea un ejemplo de implementaci贸n de l贸gica de evaluaci贸n personalizada: [custom-eval.md](docs/custom-eval.md).

Si cree que tiene una evaluaci贸n interesante, abra un PR con su contribuci贸n. El personal de OpenAI revisa activamente estas evaluaciones al considerar mejoras para los pr贸ximos modelos.

____________________
 Por tiempo limitado, otorgaremos acceso a GPT-4 a quienes contribuyan con evaluaciones de alta calidad. Siga las instrucciones mencionadas anteriormente y tenga en cuenta que se ignorar谩n los env铆os de spam o de baja calidad锔

Se otorgar谩 acceso a la direcci贸n de correo electr贸nico asociada con una evaluaci贸n aceptada. Debido al gran volumen, no podemos otorgar acceso a ning煤n correo electr贸nico que no sea el utilizado para la solicitud de extracci贸n.
____________________

## Configuraci贸n

Para ejecutar evaluaciones, deber谩 configurar y especificar su clave API de OpenAI. Puede generar uno en <https://platform.openai.com/account/api-keys>. Despu茅s de obtener una clave API, especif铆quela usando la variable de entorno `OPENAI_API_KEY`. **Tenga en cuenta los [costos](https://openai.com/pricing) asociados con el uso de la API al ejecutar evaluaciones.**

**Versi贸n m铆nima requerida: Python 3.9**

### Descargando evaluaciones

Nuestro registro Evals se almacena mediante [Git-LFS](https://git-lfs.com/). Una vez que haya descargado e instalado LFS, puede obtener las evaluaciones con:
```sh
git lfs buscar --todos
tirar de git lfs
```

Es posible que solo desee obtener datos para una evaluaci贸n seleccionada. Puede lograr esto a trav茅s de:
```sh
git lfs fetch --include=evals/registry/data/${tu evaluaci贸n}
tirar de git lfs
```

### Haciendo evaluaciones

Si va a crear evaluaciones, le sugerimos que clone este repositorio directamente desde GitHub e instale los requisitos con el siguiente comando:

```sh
pip instalar -e.
```

Usando `-e`, los cambios que realice en su evaluaci贸n se reflejar谩n inmediatamente sin tener que volver a instalar.

### Ejecutando evaluaciones

Si no desea contribuir con nuevas evaluaciones, sino simplemente ejecutarlas localmente, puede instalar el paquete de evaluaciones a trav茅s de pip:

```sh
evaluaciones de instalaci贸n de pip
```

Le brindamos la opci贸n de registrar los resultados de su evaluaci贸n en una base de datos de Snowflake, si tiene una o desea configurar una. Para esta opci贸n, deber谩 especificar adem谩s las variables de entorno `SNOWFLAKE_ACCOUNT`, `SNOWFLAKE_DATABASE`, `SNOWFLAKE_USERNAME` y `SNOWFLAKE_PASSWORD`.

## PREGUNTAS MS FRECUENTES

驴Tiene alg煤n ejemplo de c贸mo construir una evaluaci贸n de principio a fin?

- 隆S铆! Estos est谩n en la carpeta `examples`. Le recomendamos que tambi茅n lea [build-eval.md](docs/build-eval.md) para obtener una comprensi贸n m谩s profunda de lo que sucede en estos ejemplos.

驴Tiene alg煤n ejemplo de evaluaciones implementadas de m煤ltiples maneras diferentes?

- 隆S铆! En particular, consulte `evals/registry/evals/coqa.yaml`. Hemos implementado peque帽os subconjuntos del conjunto de datos [CoQA](https://stanfordnlp.github.io/coqa/) para varias plantillas de evaluaci贸n para ayudar a ilustrar las diferencias.

Cuando ejecuto una evaluaci贸n, a veces se cuelga al final (despu茅s del informe final). 驴Qu茅 est谩 sucediendo?

- Este es un problema conocido, pero deber铆a poder interrumpirlo de manera segura y la evaluaci贸n deber铆a finalizar inmediatamente despu茅s.

Hay mucho c贸digo y solo quiero hacer una evaluaci贸n r谩pida. 驴Ayuda? O,

Soy un ingeniero r谩pido de clase mundial. Elijo no codificar. 驴C贸mo puedo aportar mi sabidur铆a?

- Si sigue una [plantilla de evaluaci贸n] existente (docs/eval-templates.md) para crear una evaluaci贸n b谩sica o graduada por modelo, 隆no necesita escribir ning煤n c贸digo de evaluaci贸n! Simplemente proporcione sus datos en formato JSON y especifique sus par谩metros de evaluaci贸n en YAML. [build-eval.md](docs/build-eval.md) lo gu铆a a trav茅s de estos pasos, y puede complementar estas instrucciones con los cuadernos de Jupyter Notebook en la carpeta `examples` para ayudarlo a comenzar r谩pidamente. Sin embargo, tenga en cuenta que una buena evaluaci贸n inevitablemente requerir谩 una reflexi贸n cuidadosa y una experimentaci贸n rigurosa.

## Descargo de responsabilidad

Al contribuir con Evals, usted acepta que la l贸gica y los datos de su evaluaci贸n est茅n bajo la misma licencia del MIT que este repositorio. Debe tener los derechos adecuados para cargar cualquier dato utilizado en una evaluaci贸n. OpenAI se reserva el derecho de utilizar estos datos en futuras mejoras de servicio de nuestro producto. Las contribuciones a OpenAI Evals estar谩n sujetas a nuestras Pol铆ticas de uso habituales: https://platform.openai.com/docs/usage-pol
